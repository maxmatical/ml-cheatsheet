{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBM Experiments\n",
    "\n",
    "Setting up GBM experiments to run hyperparameter optimization + model training using\n",
    "- catboost\n",
    "- lightgbm\n",
    "- xgboost\n",
    "\n",
    "On some tabular data \n",
    "\n",
    "So experiments can be extended to score combiner data once we get it in order to determine the gradient boostling library to use for our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set optuna variables\n",
    "n_trials = 25\n",
    "timeout = 30  # time out in minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data\n",
    "\n",
    "- create train/val split based off of a col \n",
    "- separate categorical from continuous features\n",
    "- create `train_df`, `val_df`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data comes from the kaggle titanic dataset, but can be any dataset\n",
    "\n",
    "to download the data, create a kaggle account and run \n",
    "`kaggle competitions download -c titanic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\")  # path to train csv should be a part of the config yaml\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill nas \n",
    "df.fillna(-999, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all these should be in the config yaml name\n",
    "split_by_col_name = \"PassengerId\"  # used because we want to split by questions in score combbiner\n",
    "cat_feat_names = [\"Pclass\", \"Sex\", \"Embarked\"]\n",
    "cont_feat_names = [\"Age\", \"SibSp\", \"Fare\"]\n",
    "label = \"Survived\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in cat_feat_names:\n",
    "    df[feat] = df[feat].astype(\"category\")\n",
    "\n",
    "for feat in cont_feat_names:\n",
    "    df[feat] = df[feat].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only take features we care about\n",
    "all_cols = [label, *cat_feat_names, *cont_feat_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, (891, 12))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"PassengerId\"]), df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 7)\n"
     ]
    }
   ],
   "source": [
    "# only take cols we care about\n",
    "df = df[all_cols]\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this case, i am using just a simple train/val split, for a k-fold cross validation, do the following\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "kf.get_n_splits(df[split_by_col_name].unique())\n",
    "```\n",
    "when running optuna trial, want to train k models with the same hyperparams (1 model on each fold) and average the val accuracies\n",
    "\n",
    "don't save the best model found with kfold cv. return the best parameters, then fit the model on **all** the training data as the final model\n",
    "```\n",
    "def objective(trial):\n",
    "    avg_acc = []\n",
    "    for train_ids, val_ids in kf.split(df[split_by_col_name].unique()):\n",
    "        train_df, val_df = df[df[split_by_col_name].isin(train_ids)], df[df[split_by_col_name].isin(val_ids)]\n",
    "\n",
    "        rest of the code for data prep + model fitting goes here\n",
    "        \n",
    "        avg_acc.append(accuracy)\n",
    "    return sum(avg_acc)/len(avg_acc)\n",
    "\n",
    "```\n",
    "\n",
    "once you run the hpo, use `study.best_params` to get the best found hyperparams, then retrain your model using the best params on the **entire df (no split)**\n",
    "eg\n",
    "```\n",
    "x, y = df[df.columns.drop(label)], df[label]\n",
    "cat_feat_idxs = np.where(x_train.dtypes != np.float)[0]\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    border_count=254,\n",
    "    **study.best_params\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x_train, y_train,\n",
    "    cat_features=cat_feat_idxs,\n",
    "    eval_set=(x_val, y_val),\n",
    "    verbose=0\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it's important to know when to use kfold cv\n",
    "\n",
    "we want to look at the distribution of classes between the train/val split. make sure if we do kfold cv that\n",
    "- the classe distribution between train/val split are same\n",
    "- no data leakage between the train/val split (this is why in score combiner we're splitting by questions, not just randomly)\n",
    "\n",
    "- The above applies to doing a random train/val split as well\n",
    "- if those requirements are not satisfied by a random split, then we want to consider manually constructing a validation dataset from the data, and not running any kfold cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 12) (179, 12)\n",
      "(712, 7) (179, 7)\n"
     ]
    }
   ],
   "source": [
    "# splitting data by column value\n",
    "train_ids, val_ids = train_test_split(df[split_by_col_name].unique(), test_size=0.2)\n",
    "\n",
    "train_df, val_df = df[df[split_by_col_name].isin(train_ids)], df[df[split_by_col_name].isin(val_ids)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting train_df, val_df into x_train, y_train, x_val, y_val\n",
    "x_train, y_train = train_df[train_df.columns.drop(label)], train_df[label]\n",
    "\n",
    "x_val, y_val = val_df[val_df.columns.drop(label)], val_df[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pclass      category\n",
      "Sex         category\n",
      "Embarked    category\n",
      "Age          float64\n",
      "SibSp        float64\n",
      "Fare         float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_train.dtypes)\n",
    "cat_feat_idxs = np.where(x_train.dtypes != np.float)[0]\n",
    "cat_feat_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost\n",
    "\n",
    "https://github.com/catboost/tutorials/blob/master/python_tutorial.ipynb\n",
    "\n",
    "https://www.kaggle.com/satorushibata/optimize-catboost-hyperparameter-with-optuna-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x16133f820>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# model = CatBoostClassifier(\n",
    "#     custom_loss=[metrics.Accuracy()],\n",
    "#     random_seed=42,\n",
    "#     logging_level='Silent'\n",
    "# )\n",
    "\n",
    "# model.fit(\n",
    "#     x_train, y_train,\n",
    "#     cat_features=cat_feat_idxs,\n",
    "#     eval_set=(x_val, y_val),\n",
    "#     verbose=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-24 17:45:56,919]\u001b[0m A new study created in memory with name: no-name-40b80f69-2c14-4362-9a2e-f2763a34d818\u001b[0m\n",
      "\u001b[32m[I 2022-01-24 17:45:58,192]\u001b[0m Trial 0 finished with value: 0.8044692737430168 and parameters: {'iterations': 212, 'depth': 11, 'random_strength': 87, 'bagging_temperature': 0.4444868603650943, 'learning_rate': 0.10200169286367641, 'objective': 'CrossEntropy', 'l2_leaf_reg': 2.0398002633861805}. Best is trial 0 with value: 0.8044692737430168.\u001b[0m\n",
      "\u001b[32m[I 2022-01-24 17:45:58,673]\u001b[0m Trial 1 finished with value: 0.7988826815642458 and parameters: {'iterations': 200, 'depth': 12, 'random_strength': 48, 'bagging_temperature': 0.04667920409334895, 'learning_rate': 0.0029610102225529015, 'objective': 'Logloss', 'l2_leaf_reg': 2.102125802721445e-06}. Best is trial 0 with value: 0.8044692737430168.\u001b[0m\n",
      "\u001b[32m[I 2022-01-24 17:45:59,185]\u001b[0m Trial 2 finished with value: 0.7877094972067039 and parameters: {'iterations': 242, 'depth': 11, 'random_strength': 26, 'bagging_temperature': 0.0657698801083876, 'learning_rate': 0.0012454252220416337, 'objective': 'Logloss', 'l2_leaf_reg': 1.752664896502559e-06}. Best is trial 0 with value: 0.8044692737430168.\u001b[0m\n",
      "\u001b[32m[I 2022-01-24 17:45:59,342]\u001b[0m Trial 3 finished with value: 0.7988826815642458 and parameters: {'iterations': 129, 'depth': 9, 'random_strength': 30, 'bagging_temperature': 46.3954209567499, 'learning_rate': 0.04175429613939908, 'objective': 'CrossEntropy', 'l2_leaf_reg': 8.985636358108276e-05}. Best is trial 0 with value: 0.8044692737430168.\u001b[0m\n",
      "\u001b[32m[I 2022-01-24 17:45:59,476]\u001b[0m Trial 4 finished with value: 0.7877094972067039 and parameters: {'iterations': 114, 'depth': 12, 'random_strength': 27, 'bagging_temperature': 0.6382315116059868, 'learning_rate': 0.009264520398016035, 'objective': 'CrossEntropy', 'l2_leaf_reg': 2.313923603021893e-08}. Best is trial 0 with value: 0.8044692737430168.\u001b[0m\n",
      "\u001b[32m[I 2022-01-24 17:45:59,716]\u001b[0m Trial 5 finished with value: 0.7988826815642458 and parameters: {'iterations': 204, 'depth': 6, 'random_strength': 53, 'bagging_temperature': 1.9043914126410604, 'learning_rate': 0.12849763488535748, 'objective': 'Logloss', 'l2_leaf_reg': 1.6191761422285424e-08}. Best is trial 0 with value: 0.8044692737430168.\u001b[0m\n",
      "\u001b[32m[I 2022-01-24 17:46:00,720]\u001b[0m Trial 6 finished with value: 0.7877094972067039 and parameters: {'iterations': 275, 'depth': 9, 'random_strength': 34, 'bagging_temperature': 0.02534215767633544, 'learning_rate': 0.2972034728853156, 'objective': 'CrossEntropy', 'l2_leaf_reg': 8.813272768539513e-06}. Best is trial 0 with value: 0.8044692737430168.\u001b[0m\n",
      "\u001b[32m[I 2022-01-24 17:46:01,034]\u001b[0m Trial 7 finished with value: 0.7932960893854749 and parameters: {'iterations': 243, 'depth': 8, 'random_strength': 25, 'bagging_temperature': 8.33962015112205, 'learning_rate': 0.0031030912758659174, 'objective': 'Logloss', 'l2_leaf_reg': 3.725424253043366}. Best is trial 0 with value: 0.8044692737430168.\u001b[0m\n",
      "\u001b[32m[I 2022-01-24 17:46:01,158]\u001b[0m Trial 8 finished with value: 0.7988826815642458 and parameters: {'iterations': 193, 'depth': 5, 'random_strength': 68, 'bagging_temperature': 1.1788594007198356, 'learning_rate': 0.10286229023872893, 'objective': 'Logloss', 'l2_leaf_reg': 9.693867249405721e-08}. Best is trial 0 with value: 0.8044692737430168.\u001b[0m\n",
      "\u001b[32m[I 2022-01-24 17:46:01,415]\u001b[0m Trial 9 finished with value: 0.7932960893854749 and parameters: {'iterations': 263, 'depth': 5, 'random_strength': 68, 'bagging_temperature': 1.3862062002512763, 'learning_rate': 0.07174465692969892, 'objective': 'CrossEntropy', 'l2_leaf_reg': 2.947682517801834e-08}. Best is trial 0 with value: 0.8044692737430168.\u001b[0m\n",
      "\u001b[32m[I 2022-01-24 17:46:01,630]\u001b[0m Trial 10 finished with value: 0.7988826815642458 and parameters: {'iterations': 62, 'depth': 10, 'random_strength': 100, 'bagging_temperature': 0.2968472238772329, 'learning_rate': 0.5524629851673242, 'objective': 'CrossEntropy', 'l2_leaf_reg': 0.6542941279714155}. Best is trial 0 with value: 0.8044692737430168.\u001b[0m\n",
      "\u001b[32m[I 2022-01-24 17:46:01,900]\u001b[0m Trial 11 finished with value: 0.7821229050279329 and parameters: {'iterations': 162, 'depth': 12, 'random_strength': 100, 'bagging_temperature': 0.011209214804751245, 'learning_rate': 0.012090874009016369, 'objective': 'Logloss', 'l2_leaf_reg': 0.023640386929679815}. Best is trial 0 with value: 0.8044692737430168.\u001b[0m\n",
      "\u001b[32m[I 2022-01-24 17:46:02,425]\u001b[0m Trial 12 finished with value: 0.7932960893854749 and parameters: {'iterations': 214, 'depth': 12, 'random_strength': 80, 'bagging_temperature': 0.11377335514039717, 'learning_rate': 0.0112682707668817, 'objective': 'CrossEntropy', 'l2_leaf_reg': 0.003317177895574222}. Best is trial 0 with value: 0.8044692737430168.\u001b[0m\n",
      "\u001b[32m[I 2022-01-24 17:46:03,584]\u001b[0m Trial 13 finished with value: 0.7821229050279329 and parameters: {'iterations': 160, 'depth': 11, 'random_strength': 0, 'bagging_temperature': 0.15441875864996313, 'learning_rate': 0.00197098644410744, 'objective': 'Logloss', 'l2_leaf_reg': 2.5085438219930752e-06}. Best is trial 0 with value: 0.8044692737430168.\u001b[0m\n",
      "\u001b[32m[I 2022-01-24 17:46:04,774]\u001b[0m Trial 14 finished with value: 0.7932960893854749 and parameters: {'iterations': 296, 'depth': 10, 'random_strength': 49, 'bagging_temperature': 0.02920774464385759, 'learning_rate': 0.8839092722078784, 'objective': 'Logloss', 'l2_leaf_reg': 0.00017603939579428917}. Best is trial 0 with value: 0.8044692737430168.\u001b[0m\n",
      "\u001b[32m[I 2022-01-24 17:46:05,018]\u001b[0m Trial 15 finished with value: 0.7988826815642458 and parameters: {'iterations': 215, 'depth': 7, 'random_strength': 87, 'bagging_temperature': 5.606194279593446, 'learning_rate': 0.004910559139195223, 'objective': 'CrossEntropy', 'l2_leaf_reg': 0.06789169972588446}. Best is trial 0 with value: 0.8044692737430168.\u001b[0m\n",
      "\u001b[32m[I 2022-01-24 17:46:05,185]\u001b[0m Trial 16 finished with value: 0.7932960893854749 and parameters: {'iterations': 138, 'depth': 11, 'random_strength': 48, 'bagging_temperature': 0.30456571640448843, 'learning_rate': 0.02262568518801925, 'objective': 'CrossEntropy', 'l2_leaf_reg': 5.35512154926573e-07}. Best is trial 0 with value: 0.8044692737430168.\u001b[0m\n",
      "\u001b[32m[I 2022-01-24 17:46:05,383]\u001b[0m Trial 17 finished with value: 0.7932960893854749 and parameters: {'iterations': 57, 'depth': 10, 'random_strength': 100, 'bagging_temperature': 0.4533468167065671, 'learning_rate': 0.8079527257006914, 'objective': 'CrossEntropy', 'l2_leaf_reg': 9.928307300307786}. Best is trial 0 with value: 0.8044692737430168.\u001b[0m\n",
      "\u001b[32m[I 2022-01-24 17:46:06,049]\u001b[0m Trial 18 finished with value: 0.7877094972067039 and parameters: {'iterations': 236, 'depth': 7, 'random_strength': 86, 'bagging_temperature': 6.014060643298552, 'learning_rate': 0.22495544486704244, 'objective': 'CrossEntropy', 'l2_leaf_reg': 0.09997587954621937}. Best is trial 0 with value: 0.8044692737430168.\u001b[0m\n",
      "\u001b[32m[I 2022-01-24 17:46:06,506]\u001b[0m Trial 19 finished with value: 0.7988826815642458 and parameters: {'iterations': 185, 'depth': 12, 'random_strength': 5, 'bagging_temperature': 0.011083068951463288, 'learning_rate': 0.0396564330698132, 'objective': 'Logloss', 'l2_leaf_reg': 3.0346914117479083e-05}. Best is trial 0 with value: 0.8044692737430168.\u001b[0m\n",
      "\u001b[32m[I 2022-01-24 17:46:06,694]\u001b[0m Trial 20 finished with value: 0.8044692737430168 and parameters: {'iterations': 225, 'depth': 4, 'random_strength': 87, 'bagging_temperature': 57.19411682917116, 'learning_rate': 0.005358129213097094, 'objective': 'CrossEntropy', 'l2_leaf_reg': 0.001973598595439713}. Best is trial 0 with value: 0.8044692737430168.\u001b[0m\n",
      "\u001b[32m[I 2022-01-24 17:46:06,748]\u001b[0m Trial 21 finished with value: 0.7877094972067039 and parameters: {'iterations': 53, 'depth': 4, 'random_strength': 95, 'bagging_temperature': 57.538541017995406, 'learning_rate': 0.4211240990222339, 'objective': 'CrossEntropy', 'l2_leaf_reg': 0.664736946581577}. Best is trial 0 with value: 0.8044692737430168.\u001b[0m\n",
      "\u001b[32m[I 2022-01-24 17:46:06,883]\u001b[0m Trial 22 finished with value: 0.7932960893854749 and parameters: {'iterations': 176, 'depth': 4, 'random_strength': 5, 'bagging_temperature': 16.067945239521023, 'learning_rate': 0.03173624617198829, 'objective': 'CrossEntropy', 'l2_leaf_reg': 0.0013910129792555144}. Best is trial 0 with value: 0.8044692737430168.\u001b[0m\n",
      "\u001b[32m[I 2022-01-24 17:46:07,245]\u001b[0m Trial 23 finished with value: 0.7988826815642458 and parameters: {'iterations': 228, 'depth': 11, 'random_strength': 66, 'bagging_temperature': 0.05185411265295218, 'learning_rate': 0.005493384373751811, 'objective': 'CrossEntropy', 'l2_leaf_reg': 0.001734069466847648}. Best is trial 0 with value: 0.8044692737430168.\u001b[0m\n",
      "\u001b[32m[I 2022-01-24 17:46:07,446]\u001b[0m Trial 24 finished with value: 0.8156424581005587 and parameters: {'iterations': 181, 'depth': 9, 'random_strength': 14, 'bagging_temperature': 98.64827444466418, 'learning_rate': 0.0559949172297065, 'objective': 'Logloss', 'l2_leaf_reg': 3.7640306797012666e-05}. Best is trial 24 with value: 0.8156424581005587.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 25\n",
      "Best trial:\n",
      "  Value: 0.8156424581005587\n",
      "  Params: \n",
      "    iterations: 181\n",
      "    depth: 9\n",
      "    random_strength: 14\n",
      "    bagging_temperature: 98.64827444466418\n",
      "    learning_rate: 0.0559949172297065\n",
      "    objective: Logloss\n",
      "    l2_leaf_reg: 3.7640306797012666e-05\n"
     ]
    }
   ],
   "source": [
    "# hpo \n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'iterations' : trial.suggest_int('iterations', 50, 300),                         \n",
    "        'depth' : trial.suggest_int('depth', 4, 12),                                       \n",
    "        'random_strength' :trial.suggest_int('random_strength', 0, 100),                       \n",
    "        'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n",
    "        'learning_rate' :trial.suggest_loguniform('learning_rate', 1e-3, 1),\n",
    "        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n",
    "        \"l2_leaf_reg\": trial.suggest_loguniform(\"l2_leaf_reg\", 1e-8, 10.0),\n",
    "    }\n",
    "\n",
    "\n",
    "    model = CatBoostClassifier(\n",
    "        border_count=254,\n",
    "        **params\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        cat_features=cat_feat_idxs,\n",
    "        eval_set=(x_val, y_val),\n",
    "        verbose=0\n",
    "    )\n",
    "    preds = model.predict(x_val)\n",
    "    pred_labels = np.rint(preds)\n",
    "    accuracy = accuracy_score(y_val, pred_labels)\n",
    "    \n",
    "    trial.set_user_attr(key=\"best_model\", value=model)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def callback(study, trial):\n",
    "    if study.best_trial.number == trial.number:\n",
    "        study.set_user_attr(key=\"best_model\", value=trial.user_attrs[\"best_model\"])\n",
    "\n",
    "\n",
    "# run study\n",
    "pruner = optuna.pruners.SuccessiveHalvingPruner()\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "study.optimize(objective, n_trials=n_trials, timeout=timeout*60, callbacks=[callback])\n",
    "\n",
    "# log intermediate values\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8156424581005587\n"
     ]
    }
   ],
   "source": [
    "# reload to verify matches HPO values\n",
    "best_model=study.user_attrs[\"best_model\"]\n",
    "preds = best_model.predict(x_val)\n",
    "pred_labels = np.rint(preds)\n",
    "accuracy = accuracy_score(y_val, pred_labels)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model w/ pickle\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'binary'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = \"binary\" if len(y_train.unique()) == 2 else \"multiclass\"\n",
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cat_feat_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pclass      category\n",
      "Sex         category\n",
      "Cabin       category\n",
      "Embarked    category\n",
      "Age          float64\n",
      "SibSp        float64\n",
      "Fare         float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(x_train.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "        \"objective\": obj,\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",                \n",
    "        \"seed\": 42\n",
    "    }\n",
    "dtrain = lgb.Dataset(x_train, label=y_train, categorical_feature=cat_feat_names)   \n",
    "model = lgb.train(params, dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-24 17:39:54,705]\u001b[0m A new study created in memory with name: no-name-8cd73383-a600-4769-9dcc-0451a4fb283e\u001b[0m\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-01-24 17:39:54,890]\u001b[0m Trial 0 finished with value: 0.7821229050279329 and parameters: {'num_iterations': 287, 'max_depth': 5, 'boosting': 'gbdt', 'learning_rate': 0.004967601640257454, 'lambda_l1': 0.005092660037630496, 'lambda_l2': 2.648607498217249e-08, 'num_leaves': 204, 'feature_fraction': 0.8358845199756579, 'bagging_fraction': 0.6888836168623422, 'bagging_freq': 7, 'min_data_in_leaf': 31}. Best is trial 0 with value: 0.7821229050279329.\u001b[0m\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-01-24 17:39:54,991]\u001b[0m Trial 1 finished with value: 0.8044692737430168 and parameters: {'num_iterations': 174, 'max_depth': 9, 'boosting': 'dart', 'learning_rate': 0.8558091514908599, 'lambda_l1': 3.470822881962566e-08, 'lambda_l2': 3.1256988321417203, 'num_leaves': 21, 'feature_fraction': 0.9497913370734663, 'bagging_fraction': 0.5663942977724015, 'bagging_freq': 5, 'min_data_in_leaf': 98}. Best is trial 1 with value: 0.8044692737430168.\u001b[0m\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-01-24 17:39:55,177]\u001b[0m Trial 2 finished with value: 0.7877094972067039 and parameters: {'num_iterations': 224, 'max_depth': 5, 'boosting': 'dart', 'learning_rate': 0.0038621052673259177, 'lambda_l1': 0.03705234664538656, 'lambda_l2': 1.4704299277698818e-06, 'num_leaves': 65, 'feature_fraction': 0.9636390516036926, 'bagging_fraction': 0.8941209177566176, 'bagging_freq': 5, 'min_data_in_leaf': 23}. Best is trial 1 with value: 0.8044692737430168.\u001b[0m\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-01-24 17:39:55,294]\u001b[0m Trial 3 finished with value: 0.7821229050279329 and parameters: {'num_iterations': 157, 'max_depth': 7, 'boosting': 'dart', 'learning_rate': 0.024264049746236513, 'lambda_l1': 1.963066330961964e-06, 'lambda_l2': 3.5958407843022293, 'num_leaves': 7, 'feature_fraction': 0.46410025827185253, 'bagging_fraction': 0.905208119596142, 'bagging_freq': 4, 'min_data_in_leaf': 71}. Best is trial 1 with value: 0.8044692737430168.\u001b[0m\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-01-24 17:39:55,329]\u001b[0m Trial 4 finished with value: 0.8100558659217877 and parameters: {'num_iterations': 53, 'max_depth': 11, 'boosting': 'gbdt', 'learning_rate': 0.3731362564782607, 'lambda_l1': 5.4892257336654395e-05, 'lambda_l2': 3.7473706016301196e-08, 'num_leaves': 62, 'feature_fraction': 0.583145812197479, 'bagging_fraction': 0.6212470547530244, 'bagging_freq': 7, 'min_data_in_leaf': 30}. Best is trial 4 with value: 0.8100558659217877.\u001b[0m\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-01-24 17:39:55,451]\u001b[0m Trial 5 finished with value: 0.7932960893854749 and parameters: {'num_iterations': 171, 'max_depth': 6, 'boosting': 'dart', 'learning_rate': 0.44315994895711885, 'lambda_l1': 0.36973025558221956, 'lambda_l2': 1.7804635609357573e-07, 'num_leaves': 107, 'feature_fraction': 0.4684228417946988, 'bagging_fraction': 0.8618666998969035, 'bagging_freq': 3, 'min_data_in_leaf': 85}. Best is trial 4 with value: 0.8100558659217877.\u001b[0m\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-01-24 17:39:55,510]\u001b[0m Trial 6 finished with value: 0.7541899441340782 and parameters: {'num_iterations': 206, 'max_depth': 6, 'boosting': 'gbdt', 'learning_rate': 0.0071144343182291125, 'lambda_l1': 0.020824015012010418, 'lambda_l2': 2.6601086795805814e-06, 'num_leaves': 72, 'feature_fraction': 0.936593464719965, 'bagging_fraction': 0.6362274768906616, 'bagging_freq': 1, 'min_data_in_leaf': 86}. Best is trial 4 with value: 0.8100558659217877.\u001b[0m\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-01-24 17:39:55,576]\u001b[0m Trial 7 finished with value: 0.8100558659217877 and parameters: {'num_iterations': 158, 'max_depth': 4, 'boosting': 'gbdt', 'learning_rate': 0.23157927529015807, 'lambda_l1': 0.18979102679086501, 'lambda_l2': 2.6978957539502856e-06, 'num_leaves': 121, 'feature_fraction': 0.6739318362729194, 'bagging_fraction': 0.7955966699949244, 'bagging_freq': 2, 'min_data_in_leaf': 99}. Best is trial 4 with value: 0.8100558659217877.\u001b[0m\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-01-24 17:39:55,652]\u001b[0m Trial 8 finished with value: 0.7988826815642458 and parameters: {'num_iterations': 197, 'max_depth': 5, 'boosting': 'gbdt', 'learning_rate': 0.1145333369550056, 'lambda_l1': 0.16382459810908098, 'lambda_l2': 9.11057084397454e-08, 'num_leaves': 30, 'feature_fraction': 0.43165207344572654, 'bagging_fraction': 0.8773174063430003, 'bagging_freq': 5, 'min_data_in_leaf': 62}. Best is trial 4 with value: 0.8100558659217877.\u001b[0m\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-01-24 17:39:55,711]\u001b[0m Trial 9 finished with value: 0.6536312849162011 and parameters: {'num_iterations': 229, 'max_depth': 12, 'boosting': 'gbdt', 'learning_rate': 0.0010574403225716111, 'lambda_l1': 3.7689808096069716e-08, 'lambda_l2': 0.06909768580797727, 'num_leaves': 247, 'feature_fraction': 0.9619148414659277, 'bagging_fraction': 0.4359283012493229, 'bagging_freq': 6, 'min_data_in_leaf': 75}. Best is trial 4 with value: 0.8100558659217877.\u001b[0m\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-01-24 17:39:55,805]\u001b[0m Trial 10 finished with value: 0.8268156424581006 and parameters: {'num_iterations': 56, 'max_depth': 12, 'boosting': 'gbdt', 'learning_rate': 0.07079554754989575, 'lambda_l1': 2.2565822651338365e-05, 'lambda_l2': 0.00038412835183046214, 'num_leaves': 179, 'feature_fraction': 0.6190098943391781, 'bagging_fraction': 0.45078515828584975, 'bagging_freq': 7, 'min_data_in_leaf': 5}. Best is trial 10 with value: 0.8268156424581006.\u001b[0m\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-01-24 17:39:55,884]\u001b[0m Trial 11 finished with value: 0.8156424581005587 and parameters: {'num_iterations': 50, 'max_depth': 12, 'boosting': 'gbdt', 'learning_rate': 0.07529528827561559, 'lambda_l1': 2.9751409966764665e-05, 'lambda_l2': 0.0005702086743410392, 'num_leaves': 178, 'feature_fraction': 0.612955917608917, 'bagging_fraction': 0.40044324605382103, 'bagging_freq': 7, 'min_data_in_leaf': 6}. Best is trial 10 with value: 0.8268156424581006.\u001b[0m\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-01-24 17:39:55,970]\u001b[0m Trial 12 finished with value: 0.8100558659217877 and parameters: {'num_iterations': 62, 'max_depth': 10, 'boosting': 'gbdt', 'learning_rate': 0.054395073837307455, 'lambda_l1': 3.446707632015303e-05, 'lambda_l2': 0.0007524363675354281, 'num_leaves': 184, 'feature_fraction': 0.6256031178542121, 'bagging_fraction': 0.4121076102553876, 'bagging_freq': 7, 'min_data_in_leaf': 6}. Best is trial 10 with value: 0.8268156424581006.\u001b[0m\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-01-24 17:39:56,063]\u001b[0m Trial 13 finished with value: 0.8100558659217877 and parameters: {'num_iterations': 102, 'max_depth': 12, 'boosting': 'gbdt', 'learning_rate': 0.02453455132879405, 'lambda_l1': 1.3659787007537792e-06, 'lambda_l2': 0.0004087185891392309, 'num_leaves': 175, 'feature_fraction': 0.7582103299904176, 'bagging_fraction': 0.5011014038920398, 'bagging_freq': 6, 'min_data_in_leaf': 11}. Best is trial 10 with value: 0.8268156424581006.\u001b[0m\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-01-24 17:39:56,137]\u001b[0m Trial 14 finished with value: 0.8156424581005587 and parameters: {'num_iterations': 95, 'max_depth': 10, 'boosting': 'gbdt', 'learning_rate': 0.07522388994871163, 'lambda_l1': 0.0005272865240337291, 'lambda_l2': 0.007872765516277412, 'num_leaves': 243, 'feature_fraction': 0.5460515926589773, 'bagging_fraction': 0.4025797224653902, 'bagging_freq': 6, 'min_data_in_leaf': 44}. Best is trial 10 with value: 0.8268156424581006.\u001b[0m\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-01-24 17:39:56,222]\u001b[0m Trial 15 finished with value: 0.7932960893854749 and parameters: {'num_iterations': 99, 'max_depth': 12, 'boosting': 'gbdt', 'learning_rate': 0.013078740369496208, 'lambda_l1': 1.704551933255572e-06, 'lambda_l2': 6.281524230212711e-05, 'num_leaves': 158, 'feature_fraction': 0.744157062728416, 'bagging_fraction': 0.5009686525539186, 'bagging_freq': 7, 'min_data_in_leaf': 13}. Best is trial 10 with value: 0.8268156424581006.\u001b[0m\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-01-24 17:39:56,290]\u001b[0m Trial 16 finished with value: 0.8156424581005587 and parameters: {'num_iterations': 51, 'max_depth': 11, 'boosting': 'gbdt', 'learning_rate': 0.17705074460799117, 'lambda_l1': 0.0006994398843559965, 'lambda_l2': 0.04689727119992745, 'num_leaves': 216, 'feature_fraction': 0.53599084884786, 'bagging_fraction': 0.48990525317606254, 'bagging_freq': 6, 'min_data_in_leaf': 44}. Best is trial 10 with value: 0.8268156424581006.\u001b[0m\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-01-24 17:39:56,364]\u001b[0m Trial 17 finished with value: 0.7653631284916201 and parameters: {'num_iterations': 131, 'max_depth': 9, 'boosting': 'gbdt', 'learning_rate': 0.16044297847835876, 'lambda_l1': 6.9323494328737185, 'lambda_l2': 0.06767426992988092, 'num_leaves': 214, 'feature_fraction': 0.5164429824291795, 'bagging_fraction': 0.5015958241675007, 'bagging_freq': 4, 'min_data_in_leaf': 47}. Best is trial 10 with value: 0.8268156424581006.\u001b[0m\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-01-24 17:39:56,437]\u001b[0m Trial 18 finished with value: 0.8100558659217877 and parameters: {'num_iterations': 82, 'max_depth': 10, 'boosting': 'gbdt', 'learning_rate': 0.04869408404547608, 'lambda_l1': 0.0007827993418900997, 'lambda_l2': 0.004875358000211591, 'num_leaves': 256, 'feature_fraction': 0.67101819726522, 'bagging_fraction': 0.97376390488687, 'bagging_freq': 6, 'min_data_in_leaf': 58}. Best is trial 10 with value: 0.8268156424581006.\u001b[0m\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-01-24 17:39:56,509]\u001b[0m Trial 19 finished with value: 0.8044692737430168 and parameters: {'num_iterations': 70, 'max_depth': 11, 'boosting': 'gbdt', 'learning_rate': 0.8550876869188548, 'lambda_l1': 3.3611400394072223e-07, 'lambda_l2': 0.0927310554501525, 'num_leaves': 217, 'feature_fraction': 0.4067477756326483, 'bagging_fraction': 0.5635010735870372, 'bagging_freq': 5, 'min_data_in_leaf': 38}. Best is trial 10 with value: 0.8268156424581006.\u001b[0m\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-01-24 17:39:56,603]\u001b[0m Trial 20 finished with value: 0.8212290502793296 and parameters: {'num_iterations': 125, 'max_depth': 8, 'boosting': 'gbdt', 'learning_rate': 0.26821472891532755, 'lambda_l1': 0.0029757361480043398, 'lambda_l2': 6.912377338533617e-05, 'num_leaves': 156, 'feature_fraction': 0.5279683045798373, 'bagging_fraction': 0.46716305974022193, 'bagging_freq': 6, 'min_data_in_leaf': 20}. Best is trial 10 with value: 0.8268156424581006.\u001b[0m\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-01-24 17:39:56,688]\u001b[0m Trial 21 finished with value: 0.8324022346368715 and parameters: {'num_iterations': 127, 'max_depth': 8, 'boosting': 'gbdt', 'learning_rate': 0.08943436474164031, 'lambda_l1': 2.708355509939539e-05, 'lambda_l2': 4.061434202847221e-05, 'num_leaves': 150, 'feature_fraction': 0.601396189003214, 'bagging_fraction': 0.4465814355912465, 'bagging_freq': 7, 'min_data_in_leaf': 21}. Best is trial 21 with value: 0.8324022346368715.\u001b[0m\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-01-24 17:39:56,779]\u001b[0m Trial 22 finished with value: 0.8491620111731844 and parameters: {'num_iterations': 125, 'max_depth': 7, 'boosting': 'gbdt', 'learning_rate': 0.37473808487232596, 'lambda_l1': 1.1987837919667571e-05, 'lambda_l2': 2.269126618000925e-05, 'num_leaves': 146, 'feature_fraction': 0.5992935410689467, 'bagging_fraction': 0.4545056624034441, 'bagging_freq': 7, 'min_data_in_leaf': 19}. Best is trial 22 with value: 0.8491620111731844.\u001b[0m\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-01-24 17:39:56,875]\u001b[0m Trial 23 finished with value: 0.7821229050279329 and parameters: {'num_iterations': 133, 'max_depth': 7, 'boosting': 'gbdt', 'learning_rate': 0.4568521701662479, 'lambda_l1': 9.414560338190791e-06, 'lambda_l2': 3.4965321879038236e-05, 'num_leaves': 130, 'feature_fraction': 0.7296126331527881, 'bagging_fraction': 0.5840991847865042, 'bagging_freq': 7, 'min_data_in_leaf': 19}. Best is trial 22 with value: 0.8491620111731844.\u001b[0m\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/maxtian/Desktop/mlds/env/lib/python3.9/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "\u001b[32m[I 2022-01-24 17:39:57,000]\u001b[0m Trial 24 finished with value: 0.8100558659217877 and parameters: {'num_iterations': 118, 'max_depth': 8, 'boosting': 'gbdt', 'learning_rate': 0.03451421002807874, 'lambda_l1': 0.0001299171335780873, 'lambda_l2': 1.3197424662081839e-05, 'num_leaves': 143, 'feature_fraction': 0.6109829457086743, 'bagging_fraction': 0.4549542174047834, 'bagging_freq': 7, 'min_data_in_leaf': 5}. Best is trial 22 with value: 0.8491620111731844.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 25\n",
      "Best trial:\n",
      "  Value: 0.8491620111731844\n",
      "  Params: \n",
      "    num_iterations: 125\n",
      "    max_depth: 7\n",
      "    boosting: gbdt\n",
      "    learning_rate: 0.37473808487232596\n",
      "    lambda_l1: 1.1987837919667571e-05\n",
      "    lambda_l2: 2.269126618000925e-05\n",
      "    num_leaves: 146\n",
      "    feature_fraction: 0.5992935410689467\n",
      "    bagging_fraction: 0.4545056624034441\n",
      "    bagging_freq: 7\n",
      "    min_data_in_leaf: 19\n"
     ]
    }
   ],
   "source": [
    "# hpo\n",
    "def objective(trial):\n",
    "    dtrain = lgb.Dataset(x_train, label=y_train, categorical_feature=cat_feat_names)\n",
    "    params = {\n",
    "        \"objective\": obj,\n",
    "        \"verbosity\": -1,\n",
    "        'num_iterations' : trial.suggest_int('num_iterations', 50, 300), \n",
    "        'max_depth' : trial.suggest_int('max_depth', 4, 12),   \n",
    "        \"boosting\": trial.suggest_categorical(\"boosting\", [\"gbdt\", \"dart\"]),\n",
    "        'learning_rate' :trial.suggest_loguniform('learning_rate', 1e-3, 1),\n",
    "        \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
    "        \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 5, 100),\n",
    "    }\n",
    "\n",
    "    model = lgb.train(params, dtrain)\n",
    "    preds = model.predict(x_val)\n",
    "    pred_labels = np.rint(preds)\n",
    "    accuracy = accuracy_score(y_val, pred_labels)\n",
    "    \n",
    "    trial.set_user_attr(key=\"best_model\", value=model)\n",
    "    return accuracy\n",
    "\n",
    "def callback(study, trial):\n",
    "    if study.best_trial.number == trial.number:\n",
    "        study.set_user_attr(key=\"best_model\", value=trial.user_attrs[\"best_model\"])\n",
    "\n",
    "\n",
    "# run study\n",
    "pruner = optuna.pruners.SuccessiveHalvingPruner()\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "study.optimize(objective, n_trials=n_trials, timeout=timeout*60, callbacks=[callback])\n",
    "\n",
    "# log intermediate values\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only take features we care about\n",
    "all_cols = [label, *cat_feat_names, *cont_feat_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8491620111731844\n"
     ]
    }
   ],
   "source": [
    "# reload to verify matches HPO values\n",
    "best_model=study.user_attrs[\"best_model\"]\n",
    "preds = best_model.predict(x_val)\n",
    "pred_labels = np.rint(preds)\n",
    "accuracy = accuracy_score(y_val, pred_labels)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pclass', 'Sex', 'Embarked']\n",
      "Pclass      category\n",
      "Sex         category\n",
      "Embarked    category\n",
      "Age          float64\n",
      "SibSp        float64\n",
      "Fare         float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>S</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>C</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>S</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>S</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>S</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Pclass     Sex Embarked   Age  SibSp     Fare\n",
       "0      3    male        S  22.0    1.0   7.2500\n",
       "1      1  female        C  38.0    1.0  71.2833\n",
       "2      3  female        S  26.0    0.0   7.9250\n",
       "3      1  female        S  35.0    1.0  53.1000\n",
       "4      3    male        S  35.0    0.0   8.0500"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(cat_feat_names)\n",
    "print(x_train.dtypes)\n",
    "x_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one-hot encoding\n",
    "def create_one_hot_dfs():\n",
    "    one_hot_dfs = []\n",
    "    for df in [x_train, x_val]:\n",
    "        one_hot_df = df[cat_feat_names]\n",
    "        one_hot_df = pd.get_dummies(one_hot_df, columns=cat_feat_names)\n",
    "        df_no_cat_cols = df.drop(cat_feat_names, axis=1)\n",
    "        new_df = pd.concat([df_no_cat_cols, one_hot_df], axis=1)\n",
    "        for feat in new_df.columns:\n",
    "            new_df[feat] = new_df[feat].astype(\"float64\")\n",
    "        \n",
    "        one_hot_dfs.append(new_df)\n",
    "\n",
    "    return one_hot_dfs\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new, x_val_new = create_one_hot_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 6) (712, 12)\n",
      "Age              float64\n",
      "SibSp            float64\n",
      "Fare             float64\n",
      "Pclass_1         float64\n",
      "Pclass_2         float64\n",
      "Pclass_3         float64\n",
      "Sex_female       float64\n",
      "Sex_male         float64\n",
      "Embarked_-999    float64\n",
      "Embarked_C       float64\n",
      "Embarked_Q       float64\n",
      "Embarked_S       float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_-999</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  SibSp     Fare  Pclass_1  Pclass_2  Pclass_3  Sex_female  Sex_male  \\\n",
       "0  22.0    1.0   7.2500       0.0       0.0       1.0         0.0       1.0   \n",
       "1  38.0    1.0  71.2833       1.0       0.0       0.0         1.0       0.0   \n",
       "2  26.0    0.0   7.9250       0.0       0.0       1.0         1.0       0.0   \n",
       "3  35.0    1.0  53.1000       1.0       0.0       0.0         1.0       0.0   \n",
       "4  35.0    0.0   8.0500       0.0       0.0       1.0         0.0       1.0   \n",
       "\n",
       "   Embarked_-999  Embarked_C  Embarked_Q  Embarked_S  \n",
       "0            0.0         0.0         0.0         1.0  \n",
       "1            0.0         1.0         0.0         0.0  \n",
       "2            0.0         0.0         0.0         1.0  \n",
       "3            0.0         0.0         0.0         1.0  \n",
       "4            0.0         0.0         0.0         1.0  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_train.shape, x_train_new.shape)\n",
    "print(x_train_new.dtypes)\n",
    "\n",
    "x_train_new.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(179, 6) (179, 6)\n",
      "Age              float64\n",
      "SibSp            float64\n",
      "Fare             float64\n",
      "Pclass_1         float64\n",
      "Pclass_2         float64\n",
      "Pclass_3         float64\n",
      "Sex_female       float64\n",
      "Sex_male         float64\n",
      "Embarked_-999    float64\n",
      "Embarked_C       float64\n",
      "Embarked_Q       float64\n",
      "Embarked_S       float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_-999</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-999.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-999.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age  SibSp    Fare  Pclass_1  Pclass_2  Pclass_3  Sex_female  Sex_male  \\\n",
       "15   55.0    0.0  16.000       0.0       1.0       0.0         1.0       0.0   \n",
       "17 -999.0    0.0  13.000       0.0       1.0       0.0         0.0       1.0   \n",
       "18   31.0    1.0  18.000       0.0       0.0       1.0         1.0       0.0   \n",
       "24    8.0    3.0  21.075       0.0       0.0       1.0         1.0       0.0   \n",
       "26 -999.0    0.0   7.225       0.0       0.0       1.0         0.0       1.0   \n",
       "\n",
       "    Embarked_-999  Embarked_C  Embarked_Q  Embarked_S  \n",
       "15            0.0         0.0         0.0         1.0  \n",
       "17            0.0         0.0         0.0         1.0  \n",
       "18            0.0         0.0         0.0         1.0  \n",
       "24            0.0         0.0         0.0         1.0  \n",
       "26            0.0         1.0         0.0         0.0  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_val.shape, x_val.shape)\n",
    "print(x_val_new.dtypes)\n",
    "x_val_new.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-25 13:31:53,863]\u001b[0m A new study created in memory with name: no-name-0f215a4f-ddf2-4ea9-b134-9e47de7febfc\u001b[0m\n",
      "\u001b[32m[I 2022-01-25 13:31:53,925]\u001b[0m Trial 0 finished with value: 0.8100558659217877 and parameters: {'booster': 'gbtree', 'lambda': 7.246869405081062e-08, 'alpha': 2.0864713552543363e-06, 'subsample': 0.33697008050941857, 'colsample_bytree': 0.8887954343897158, 'max_depth': 5, 'min_child_weight': 3, 'eta': 0.0003896523885422125, 'gamma': 5.201055659103267e-06, 'grow_policy': 'lossguide'}. Best is trial 0 with value: 0.8100558659217877.\u001b[0m\n",
      "\u001b[32m[I 2022-01-25 13:31:54,017]\u001b[0m Trial 1 finished with value: 0.770949720670391 and parameters: {'booster': 'dart', 'lambda': 3.0743125199168593e-07, 'alpha': 3.781291921424965e-05, 'subsample': 0.7820918196780899, 'colsample_bytree': 0.8880446295636428, 'max_depth': 7, 'min_child_weight': 7, 'eta': 8.12216822799987e-08, 'gamma': 0.0004606398424415315, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.036481847706220503, 'skip_drop': 0.00016203539240547486}. Best is trial 0 with value: 0.8100558659217877.\u001b[0m\n",
      "\u001b[32m[I 2022-01-25 13:31:54,073]\u001b[0m Trial 2 finished with value: 0.8156424581005587 and parameters: {'booster': 'gbtree', 'lambda': 1.3661594854227114e-08, 'alpha': 0.007872030585079542, 'subsample': 0.9031687379426936, 'colsample_bytree': 0.41191839342566033, 'max_depth': 7, 'min_child_weight': 5, 'eta': 1.5642614561752144e-06, 'gamma': 5.949287746675908e-07, 'grow_policy': 'lossguide'}. Best is trial 2 with value: 0.8156424581005587.\u001b[0m\n",
      "\u001b[32m[I 2022-01-25 13:31:54,148]\u001b[0m Trial 3 finished with value: 0.5977653631284916 and parameters: {'booster': 'dart', 'lambda': 2.5506704102416657e-06, 'alpha': 0.040549122988144566, 'subsample': 0.5581121349274546, 'colsample_bytree': 0.8970017357272957, 'max_depth': 9, 'min_child_weight': 8, 'eta': 2.6084727942069313e-08, 'gamma': 1.1414767480737056e-07, 'grow_policy': 'lossguide', 'sample_type': 'uniform', 'normalize_type': 'forest', 'rate_drop': 0.046169583368117544, 'skip_drop': 2.834171127809216e-07}. Best is trial 2 with value: 0.8156424581005587.\u001b[0m\n",
      "\u001b[32m[I 2022-01-25 13:31:54,190]\u001b[0m Trial 4 finished with value: 0.7877094972067039 and parameters: {'booster': 'gbtree', 'lambda': 4.064180453085381e-08, 'alpha': 0.00013291400024804768, 'subsample': 0.23713316247739266, 'colsample_bytree': 0.7716107152064675, 'max_depth': 5, 'min_child_weight': 4, 'eta': 7.316976320311407e-07, 'gamma': 7.846471906028701e-06, 'grow_policy': 'depthwise'}. Best is trial 2 with value: 0.8156424581005587.\u001b[0m\n",
      "\u001b[32m[I 2022-01-25 13:31:54,234]\u001b[0m Trial 5 finished with value: 0.8212290502793296 and parameters: {'booster': 'gbtree', 'lambda': 4.892011654044119e-05, 'alpha': 4.691941156475767e-06, 'subsample': 0.7745615554244851, 'colsample_bytree': 0.373118749397851, 'max_depth': 5, 'min_child_weight': 4, 'eta': 0.22403782375537198, 'gamma': 1.0681294426650825e-06, 'grow_policy': 'depthwise'}. Best is trial 5 with value: 0.8212290502793296.\u001b[0m\n",
      "\u001b[32m[I 2022-01-25 13:31:54,290]\u001b[0m Trial 6 finished with value: 0.8044692737430168 and parameters: {'booster': 'gbtree', 'lambda': 4.956439203600978e-06, 'alpha': 3.3991719412557743e-07, 'subsample': 0.9604606330903949, 'colsample_bytree': 0.6287420827976811, 'max_depth': 7, 'min_child_weight': 5, 'eta': 8.644215630060815e-08, 'gamma': 4.62719933306811e-08, 'grow_policy': 'lossguide'}. Best is trial 5 with value: 0.8212290502793296.\u001b[0m\n",
      "\u001b[32m[I 2022-01-25 13:31:54,339]\u001b[0m Trial 7 finished with value: 0.8044692737430168 and parameters: {'booster': 'dart', 'lambda': 0.0016917180436109291, 'alpha': 1.04828473804839e-06, 'subsample': 0.2652292742847706, 'colsample_bytree': 0.31439596274636233, 'max_depth': 3, 'min_child_weight': 5, 'eta': 0.003964524218494619, 'gamma': 1.7294575689873384e-05, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 3.354255962652749e-05, 'skip_drop': 4.002388556996287e-08}. Best is trial 5 with value: 0.8212290502793296.\u001b[0m\n",
      "\u001b[32m[I 2022-01-25 13:31:54,385]\u001b[0m Trial 8 finished with value: 0.7877094972067039 and parameters: {'booster': 'gbtree', 'lambda': 6.584413634106635e-08, 'alpha': 0.9778921746242214, 'subsample': 0.7237370244473031, 'colsample_bytree': 0.8986577947344516, 'max_depth': 9, 'min_child_weight': 4, 'eta': 1.4195022521056622e-07, 'gamma': 1.6160924047442358e-05, 'grow_policy': 'lossguide'}. Best is trial 5 with value: 0.8212290502793296.\u001b[0m\n",
      "\u001b[32m[I 2022-01-25 13:31:54,442]\u001b[0m Trial 9 finished with value: 0.7932960893854749 and parameters: {'booster': 'dart', 'lambda': 8.386505913328681e-08, 'alpha': 0.5271570295589606, 'subsample': 0.5602607262865189, 'colsample_bytree': 0.5363766916458677, 'max_depth': 7, 'min_child_weight': 9, 'eta': 1.725889191284103e-07, 'gamma': 0.016202195590217912, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 8.666112694415377e-07, 'skip_drop': 9.293106798901428e-07}. Best is trial 5 with value: 0.8212290502793296.\u001b[0m\n",
      "\u001b[32m[I 2022-01-25 13:31:54,520]\u001b[0m Trial 10 finished with value: 0.7932960893854749 and parameters: {'booster': 'gbtree', 'lambda': 0.048046568154618635, 'alpha': 1.013728496556999e-08, 'subsample': 0.7313109884098394, 'colsample_bytree': 0.2334769738341983, 'max_depth': 3, 'min_child_weight': 2, 'eta': 0.7728183181993038, 'gamma': 0.5839993048013932, 'grow_policy': 'depthwise'}. Best is trial 5 with value: 0.8212290502793296.\u001b[0m\n",
      "\u001b[32m[I 2022-01-25 13:31:54,592]\u001b[0m Trial 11 finished with value: 0.8100558659217877 and parameters: {'booster': 'gbtree', 'lambda': 0.0005857905203123275, 'alpha': 0.0028576244654872547, 'subsample': 0.9983376269487727, 'colsample_bytree': 0.3964369677779331, 'max_depth': 5, 'min_child_weight': 6, 'eta': 4.585757887927296e-06, 'gamma': 4.1926511076140217e-07, 'grow_policy': 'depthwise'}. Best is trial 5 with value: 0.8212290502793296.\u001b[0m\n",
      "\u001b[32m[I 2022-01-25 13:31:54,672]\u001b[0m Trial 12 finished with value: 0.7821229050279329 and parameters: {'booster': 'gbtree', 'lambda': 0.07817689342068997, 'alpha': 0.0010798342280740202, 'subsample': 0.876948250841489, 'colsample_bytree': 0.4681996426421905, 'max_depth': 7, 'min_child_weight': 2, 'eta': 0.7712132783397547, 'gamma': 1.420246154180904e-08, 'grow_policy': 'depthwise'}. Best is trial 5 with value: 0.8212290502793296.\u001b[0m\n",
      "\u001b[32m[I 2022-01-25 13:31:54,735]\u001b[0m Trial 13 finished with value: 0.8268156424581006 and parameters: {'booster': 'gbtree', 'lambda': 1.12539313883328e-05, 'alpha': 0.018067193021280282, 'subsample': 0.8715797724178268, 'colsample_bytree': 0.2500057947924821, 'max_depth': 5, 'min_child_weight': 6, 'eta': 0.01873992573733762, 'gamma': 5.47607276650213e-07, 'grow_policy': 'depthwise'}. Best is trial 13 with value: 0.8268156424581006.\u001b[0m\n",
      "\u001b[32m[I 2022-01-25 13:31:54,793]\u001b[0m Trial 14 finished with value: 0.8044692737430168 and parameters: {'booster': 'gbtree', 'lambda': 6.774295168608796e-05, 'alpha': 5.7045108394917e-05, 'subsample': 0.818967121957012, 'colsample_bytree': 0.21277669935843738, 'max_depth': 3, 'min_child_weight': 7, 'eta': 0.032585732707789375, 'gamma': 0.0006454986989483012, 'grow_policy': 'depthwise'}. Best is trial 13 with value: 0.8268156424581006.\u001b[0m\n",
      "\u001b[32m[I 2022-01-25 13:31:54,856]\u001b[0m Trial 15 finished with value: 0.8212290502793296 and parameters: {'booster': 'gbtree', 'lambda': 4.86015636374822e-05, 'alpha': 4.57007017206463e-08, 'subsample': 0.6408175085235582, 'colsample_bytree': 0.3020393983481562, 'max_depth': 5, 'min_child_weight': 10, 'eta': 0.04427513274635511, 'gamma': 6.219809425794473e-07, 'grow_policy': 'depthwise'}. Best is trial 13 with value: 0.8268156424581006.\u001b[0m\n",
      "\u001b[32m[I 2022-01-25 13:31:54,928]\u001b[0m Trial 16 finished with value: 0.8100558659217877 and parameters: {'booster': 'gbtree', 'lambda': 4.1885106138058376e-06, 'alpha': 8.266122686511026e-06, 'subsample': 0.6504542570641143, 'colsample_bytree': 0.6338864501004309, 'max_depth': 5, 'min_child_weight': 6, 'eta': 0.07876890667259939, 'gamma': 1.573992226065665e-08, 'grow_policy': 'depthwise'}. Best is trial 13 with value: 0.8268156424581006.\u001b[0m\n",
      "\u001b[32m[I 2022-01-25 13:31:54,994]\u001b[0m Trial 17 finished with value: 0.7821229050279329 and parameters: {'booster': 'gbtree', 'lambda': 0.001588459855157636, 'alpha': 2.0379280861729935e-08, 'subsample': 0.4708144656674721, 'colsample_bytree': 0.2540665137441308, 'max_depth': 3, 'min_child_weight': 10, 'eta': 0.004127664510917907, 'gamma': 0.00014441554032612312, 'grow_policy': 'depthwise'}. Best is trial 13 with value: 0.8268156424581006.\u001b[0m\n",
      "\u001b[32m[I 2022-01-25 13:31:55,058]\u001b[0m Trial 18 finished with value: 0.8100558659217877 and parameters: {'booster': 'gbtree', 'lambda': 1.934574431681241e-05, 'alpha': 0.1351633870175931, 'subsample': 0.42251200090622354, 'colsample_bytree': 0.3136732649302066, 'max_depth': 5, 'min_child_weight': 10, 'eta': 6.546118969925464e-05, 'gamma': 2.309433161843762e-06, 'grow_policy': 'depthwise'}. Best is trial 13 with value: 0.8268156424581006.\u001b[0m\n",
      "\u001b[32m[I 2022-01-25 13:31:55,111]\u001b[0m Trial 19 finished with value: 0.7877094972067039 and parameters: {'booster': 'gbtree', 'lambda': 0.00029683151268277563, 'alpha': 7.58092352873811e-08, 'subsample': 0.6309318183924979, 'colsample_bytree': 0.20043986159253355, 'max_depth': 5, 'min_child_weight': 9, 'eta': 0.008398456899239312, 'gamma': 1.5691731153385415e-07, 'grow_policy': 'depthwise'}. Best is trial 13 with value: 0.8268156424581006.\u001b[0m\n",
      "\u001b[32m[I 2022-01-25 13:31:55,170]\u001b[0m Trial 20 finished with value: 0.8100558659217877 and parameters: {'booster': 'gbtree', 'lambda': 0.007092805932062707, 'alpha': 0.0005039456790639034, 'subsample': 0.8831196191487413, 'colsample_bytree': 0.5111479102044386, 'max_depth': 3, 'min_child_weight': 8, 'eta': 0.0005840076098275658, 'gamma': 0.005711174645302719, 'grow_policy': 'depthwise'}. Best is trial 13 with value: 0.8268156424581006.\u001b[0m\n",
      "\u001b[32m[I 2022-01-25 13:31:55,227]\u001b[0m Trial 21 finished with value: 0.8212290502793296 and parameters: {'booster': 'gbtree', 'lambda': 4.544710577875996e-05, 'alpha': 7.178123970417541e-08, 'subsample': 0.7065744636705589, 'colsample_bytree': 0.3254590698592959, 'max_depth': 5, 'min_child_weight': 3, 'eta': 0.1564909311755316, 'gamma': 1.2953665501885706e-06, 'grow_policy': 'depthwise'}. Best is trial 13 with value: 0.8268156424581006.\u001b[0m\n",
      "\u001b[32m[I 2022-01-25 13:31:55,284]\u001b[0m Trial 22 finished with value: 0.8156424581005587 and parameters: {'booster': 'gbtree', 'lambda': 8.159231147948045e-07, 'alpha': 1.0001487972983236e-07, 'subsample': 0.6570189504580999, 'colsample_bytree': 0.29999137167858586, 'max_depth': 5, 'min_child_weight': 3, 'eta': 0.027117049704830653, 'gamma': 4.4338128837704524e-05, 'grow_policy': 'depthwise'}. Best is trial 13 with value: 0.8268156424581006.\u001b[0m\n",
      "\u001b[32m[I 2022-01-25 13:31:55,344]\u001b[0m Trial 23 finished with value: 0.8100558659217877 and parameters: {'booster': 'gbtree', 'lambda': 2.4178956622122016e-05, 'alpha': 6.083573564313023e-06, 'subsample': 0.8028789795520984, 'colsample_bytree': 0.3794779601287561, 'max_depth': 5, 'min_child_weight': 4, 'eta': 0.24171489497698628, 'gamma': 2.0529433771885052e-07, 'grow_policy': 'depthwise'}. Best is trial 13 with value: 0.8268156424581006.\u001b[0m\n",
      "\u001b[32m[I 2022-01-25 13:31:55,404]\u001b[0m Trial 24 finished with value: 0.8268156424581006 and parameters: {'booster': 'gbtree', 'lambda': 0.00020907842659227625, 'alpha': 4.3921110175318836e-07, 'subsample': 0.7219789304764882, 'colsample_bytree': 0.38349955638323263, 'max_depth': 5, 'min_child_weight': 3, 'eta': 0.3261565362532402, 'gamma': 2.2741302350364784e-06, 'grow_policy': 'depthwise'}. Best is trial 13 with value: 0.8268156424581006.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 25\n",
      "Best trial:\n",
      "  Value: 0.8268156424581006\n",
      "  Params: \n",
      "    booster: gbtree\n",
      "    lambda: 1.12539313883328e-05\n",
      "    alpha: 0.018067193021280282\n",
      "    subsample: 0.8715797724178268\n",
      "    colsample_bytree: 0.2500057947924821\n",
      "    max_depth: 5\n",
      "    min_child_weight: 6\n",
      "    eta: 0.01873992573733762\n",
      "    gamma: 5.47607276650213e-07\n",
      "    grow_policy: depthwise\n"
     ]
    }
   ],
   "source": [
    "# hpo\n",
    "\n",
    "def objective(trial):\n",
    "    dtrain = xgb.DMatrix(x_train_new, label=y_train)\n",
    "    dvalid = xgb.DMatrix(x_val_new, label=y_val)\n",
    "\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        # defines booster, gblinear for linear functions.\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"dart\"]),\n",
    "        # L2 regularization weight.\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        # L1 regularization weight.\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        # maximum depth of the tree, signifies complexity of the tree.\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        # defines how selective algorithm is.\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "    model = xgb.train(param, dtrain)\n",
    "    preds = model.predict(dvalid)\n",
    "    pred_labels = np.rint(preds)\n",
    "    accuracy = accuracy_score(y_val, pred_labels)\n",
    "    trial.set_user_attr(key=\"best_model\", value=model)\n",
    "    return accuracy\n",
    "\n",
    "def callback(study, trial):\n",
    "    if study.best_trial.number == trial.number:\n",
    "        study.set_user_attr(key=\"best_model\", value=trial.user_attrs[\"best_model\"])\n",
    "\n",
    "\n",
    "# run study\n",
    "pruner = optuna.pruners.SuccessiveHalvingPruner()\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "study.optimize(objective, n_trials=n_trials, timeout=timeout*60, callbacks=[callback])\n",
    "\n",
    "# log intermediate values\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8268156424581006\n"
     ]
    }
   ],
   "source": [
    "# reload to verify matches HPO values\n",
    "best_model=study.user_attrs[\"best_model\"]\n",
    "# dvalid = xgb.DMatrix(x_val_new, label=y_val)\n",
    "dvalid = xgb.DMatrix(x_val_new)\n",
    "preds = best_model.predict(dvalid)\n",
    "pred_labels = np.rint(preds)\n",
    "accuracy = accuracy_score(y_val, pred_labels)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ec0c8b54dd71d23fa3df8b913df2ad7215aa87aea639403b839dc3ee0b5f212e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('3.9.7': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
